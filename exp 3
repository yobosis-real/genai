import torch
import torch.nn as nn
import torch.optim as optim
import random

logs = [
    "process injection detected",
    "suspicious dll loaded",
    "outbound traffic blacklisted domain",
    "privilege escalation attempt",
    "keylogger activity found"
]

vocab = list(set(" ".join(logs).split()))
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}
vocab_size = len(vocab)

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, 8)
        self.fc = nn.Linear(8, vocab_size)

    def forward(self, x):
        return self.fc(self.embed(x))

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, 8)
        self.fc = nn.Linear(8, 1)

    def forward(self, x):
        return torch.sigmoid(self.fc(self.embed(x)))

G = Generator()
D = Discriminator()

g_opt = optim.Adam(G.parameters(), lr=0.01)
d_opt = optim.Adam(D.parameters(), lr=0.01)

for _ in range(100):
    real = torch.tensor(
        [word2idx[random.choice(log.split())] for log in logs]
    )

    noise = torch.randint(0, vocab_size, real.shape)
    fake = G(noise).argmax(dim=1)

    # Train Discriminator
    d_real = D(real)
    d_fake = D(fake.detach())
    d_loss = -(torch.log(d_real) + torch.log(1 - d_fake)).mean()

    d_opt.zero_grad()
    d_loss.backward()
    d_opt.step()

    # Train Generator
    g_loss = -torch.log(D(fake)).mean()
    g_opt.zero_grad()
    g_loss.backward()
    g_opt.step()

out = torch.randint(0, vocab_size, (1,))
word = idx2word[G(out).argmax().item()]
print("Generated log:", word)
